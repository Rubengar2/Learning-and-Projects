{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) for MNIST Classification\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will explore the implementation of a Convolutional Neural Network (CNN) for the classification of the MNIST dataset. The MNIST dataset is a widely-used benchmark dataset in the field of machine learning, consisting of handwritten digits.\n",
    "\n",
    "## Objective\n",
    "The main objective of this notebook is to demonstrate the effectiveness of CNNs in classifying handwritten digits and to provide a step-by-step guide on building and training a CNN model using the TensorFlow library.\n",
    "\n",
    "## Author\n",
    "- **Jose Ruben Garcia Garcia**\n",
    "- March 2024\n",
    "\n",
    "## Reference\n",
    "- Practical Machine Learning Python Problems Solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation\n",
    "\n",
    "## Model Architecture and Parameters\n",
    "The CNN model consists of several layers, each serving a specific purpose:\n",
    "\n",
    "- **Convolutional Layers (`Conv2D`)**: Two convolutional layers with 32 and 64 filters, respectively, are used to extract features from the input images. These layers employ the ReLU activation function to introduce non-linearity.\n",
    "  \n",
    "- **MaxPooling Layers (`MaxPooling2D`)**: After each convolutional layer, a max-pooling layer with a 2x2 kernel size is applied to downsample the features and reduce computational complexity.\n",
    "  \n",
    "- **Dropout Layers (`Dropout`)**: Dropout layers are included to prevent overfitting by randomly deactivating neurons during training. Dropout rates of 0.25 and 0.5 are used after the convolutional and fully connected layers, respectively.\n",
    "  \n",
    "- **Fully Connected Layers (`Dense`)**: Two fully connected layers with ReLU activation are added for classification. The final layer uses the softmax activation function to output probabilities for each class.\n",
    "\n",
    "## Model Compilation and Training\n",
    "The model is compiled using categorical cross-entropy loss and the Adadelta optimizer. The training process involves feeding the training data through the model for a specified number of epochs, with a batch size of 128. The model's performance is monitored using validation data to ensure generalization.\n",
    "\n",
    "## Evaluation\n",
    "After training, the model is evaluated using the test dataset to assess its performance on unseen data. The choice of evaluation metrics, including test loss and accuracy, was deliberate. Test loss provides a quantitative measure of the model's predictive error, allowing us to understand how well the model is performing in terms of minimizing prediction errors. On the other hand, accuracy measures the proportion of correctly classified instances, providing insights into the overall predictive capabilities of the model. By analyzing these metrics, we gain a comprehensive understanding of the model's performance and its ability to generalize to new, unseen data. This rigorous evaluation process is essential for assessing the model's reliability and guiding further improvements\n",
    "\n",
    "## Conclusion\n",
    "the development of this Convolutional Neural Network (CNN) model for MNIST classification has yielded valuable insights into the intricacies of image classification and deep learning methodologies. Through the iterative process of model design, training, and evaluation, several key conclusions emerge. Firstly, the effectiveness of CNN architectures in extracting hierarchical features from images is underscored, showcasing their robustness in handling complex data structures. Additionally, the significance of preprocessing techniques such as normalization and data augmentation in enhancing model performance becomes apparent, highlighting the importance of data preparation in machine learning workflows. Furthermore, the impact of hyperparameter tuning and optimization strategies on model convergence and generalization is evident, emphasizing the need for systematic experimentation and fine-tuning. Overall, this project not only demonstrates my proficiency in implementing advanced machine learning techniques but also underscores the importance of methodical approach and critical analysis in model development.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 9\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
