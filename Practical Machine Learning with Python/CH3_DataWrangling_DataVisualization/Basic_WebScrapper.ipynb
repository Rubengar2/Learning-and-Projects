{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02a148d-aacb-4303-a9eb-3a49bc758933",
   "metadata": {},
   "source": [
    "## CHAPTER 3 PROCESSING, WRANGLING AND DATA VISUALIZATION.\n",
    "### WEB SCRAPPING\n",
    "#### The book encouraged us to create a web scrapper in order to know the basics of this kind of task essencials for every DS\n",
    "\n",
    "##### *Jose Ruben Garcia Garcia*\n",
    "##### *Frebuary 2024*\n",
    "##### *Reference: Practical Machine learning python problems solver*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198ab2e-fd8b-4d21-a43c-ef109ce14437",
   "metadata": {},
   "source": [
    "### Basic Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9cbe86d-61a9-45fc-a219-abb032972ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As a first task we will identify the URL that would be scrapped\n",
    "#We selected a url from aprres that it's https://www.apress.com/in/blog/all-blog-posts/gradient-descent-optimization/15512052\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090953ac-b4b2-409e-9673-29f3f180b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def extract_blog_content(content):\n",
    "    \"\"\"This function extracts blog post content using regex\n",
    "\n",
    "    Args:\n",
    "        content (request.content): String content returned from requests.get\n",
    "\n",
    "    Returns:\n",
    "        str: string content as per regex match\n",
    "\n",
    "    \"\"\"\n",
    "    content_pattern = re.compile(r'<div class=\"cms-richtext\">(.*?)</div>')\n",
    "    result = re.findall(content_pattern, content)\n",
    "    return result[0] if result else \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261a55f2-7751-4c13-ac7c-9e1d776387d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Apress.com for required blog post...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    \n",
    "    base_url = \"https://www.apress.com/in/blog/all-blog-posts\"\n",
    "    blog_suffix = \"/gradient-descent-optimization/15512052\"\n",
    "    \n",
    "    print(\"Crawling Apress.com for required blog post...\\n\\n\")    \n",
    "    \n",
    "    response = requests.get(base_url+blog_suffix)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "        content = content.replace(\"\\n\", '')\n",
    "        blog_post_content = extract_blog_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0312cdd-2c66-441c-89f4-49949887d1b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><!--[if lt IE 7]> <html lang=\"en\" class=\"no-js ie6 lt-ie9 lt-ie8\"> <![endif]--><!--[if IE 7]> <html lang=\"en\" class=\"no-js ie7 lt-ie9 lt-ie8\"> <![endif]--><!--[if IE 8]> <html lang=\"en\" class=\"no-js ie8 lt-ie9\"> <![endif]--><!--[if IE 9]> <html lang=\"en\" class=\"no-js ie9\"> <![endif]--><!--[if gt IE 9]><!--> <html lang=\"en\" class=\"no-js\"> <!--<![endif]--><head><meta http-equiv=\"x-ua-compatible\" content=\"IE=edge\"><script type=\"text/javascript\" src=\"/spcom/js/vendor/googleapis/ajax/libs/jquery/1.9.1/jquery.min.js\"></script><script type=\"text/javascript\" id=\"angular-script\" src=\"/spcom/js/vendor/googleapis/ajax/libs/angularjs/1.2.17/angular.min.js\"></script><script type=\"text/javascript\" id=\"script--165730135\" src=\"/spcom/min/prod.js?r=0.102.0\"></script><link rel=\"stylesheet\" type=\"text/css\" href=\"/spcom/min/modern_sprcom-cms-frontend_apress.css?r=0.102.0\" /><!--[if (lt IE 9) & (!IEMobile)]><link rel=\"stylesheet\" type=\"text/css\" href=\"/spcom/min/ielt9_sprcom-cms-frontend_apress.css?r=0.102.0\" media=\"screen\" /><![endif]--><link rel=\"stylesheet\" type=\"text/css\" href=\"/spcom/min/print.css?r=0.102.0\" media=\"print\" /><link rel=\"stylesheet\" type=\"text/css\" href=\"/spcom/css/vendor/font-awesome.min.css?r=0.102.0\" /><!--[if lt IE 9]><script type=\"text/javascript\" id=\"ielt9js\" charset=\"utf-8\" src=\"/spcom/min/ielt9.js\"></script><![endif]--><script type=\"text/javascript\" id=\"cm-context-url\">/*<![CDATA[*/var cmContextUrl = \\'/apress-in/in/blog/all-blog-posts\\';/*]]>*/</script><title>Gradient Descent Optimization methods from Deep Learning Perspective</title><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><link rel=\"icon\" href=\"../../../../favicon.ico\" type=\"image/ico\"><link rel=\"shortcut icon\" href=\"../../../../favicon.ico\" type=\"image/x-icon\"><script>/*<![CDATA[*/document.documentElement.className = document.documentElement.className.replace(\\'no-js\\', \\'js\\');/*]]>*/</script><script type=\"text/javascript\">dataLayer = [{\\'geo-country-code\\' : \\'MX\\'}];</script><script src=\"https://cmp.apress.com/production_live/en/consent-bundle-71-latest.js\"></script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':new Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\'https://collect.apress.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);})(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-W7J2NZR\\');</script><!-- End Google Tag Manager --><style>/*<![CDATA[*/.no-js .product-graphic .lazy {display: block;}.link-image p {color: #333333;}.link-image .look-inside-badge p {color: #ffffff;}.cms-teaser a p {color: #333333;}.cms-row-multiline-6 .product-teaser .product-information {display: none;}/*]]>*/</style><meta name=\"description\" content=\"Read our blog\"><meta http-equiv=\"content-language\" content=\"en\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta property=\"og:title\" content=\"Gradient Descent Optimization methods from Deep Learning Perspective\"><meta property=\"og:description\" content=\"Read our blog\"><meta property=\"og:site_name\" content=\"www.apress.com\"><meta property=\"og:type\" content=\"website\"><meta property=\"og:image\" content=\"https://resource-cms.springernature.com/springer-cms/rest/v1/content/16537448/data/v5\"><meta property=\"fb:admins\" content=\"100001154999617\"><meta property=\"fb:pages\" content=\"341168879741, 143932312346551, 199235833463190, 145327659223282, 304238573037854, 346209515556394, 99912541035, 108064145914608, 370429855420, 170400356321916, 273358886101252, 370987992963178, 1565916643629420, 424373190938938, 152586274808123, 152749531404582, 274648772592090, 168287986591458, 284820978207661, 121336764582486, 174395066291972, 124330454291947, 58364190265, 114336798618606, 177450032338982, 460115770673632, 526326927441845, 401070179903991, 35223474166, 2055004118104490, 227993570999679, 121053321387843, 146393288729580\"><link rel=\"alternate\" href=\"/gp/blog/all-blog-posts/gradient-descent-optimization/15512052\" hreflang=\"en\"><script type=\"text/javascript\" id=\"trackedAffiliates\">/*<![CDATA[*/var trackedAffiliates = [\\'PPC.Google AdWords.3.EPR653.SM\\', \\'Affiliate.CommissionJunction.3.EPR868-EN\\', \\'PDM.Soquero.3.EPR653\\', \\'PPC.Google AdWords.EPR653-GoogleShopping_Product_EN\\', \\'PPC.Google AdWords.3.EPR653.DAL\\', \\'Affiliate.Zanox.3.EPR868-DE\\', \\'PPC.Google AdWords.3.EPR653.GMT\\', \\'Affiliate.CommissionJunction.3.EPR868-DE\\', \\'PPC.Google AdWords.EPR653-GoogleShopping_Product_CH\\', \\'PDM.ChannelAdvisor.3.EPR653\\', \\'Banner.Banner RTB.3.EPR653.Sociomantic\\', \\'PPC.Google AdWords.EPR653-GoogleShopping_Product_DE\\', \\'Affiliate.Zanox.3.EPR868-EN\\', \\'PPC.Google AdWords.3.EPR653.IT\\', \\'PPC.Google AdWords.3.EPR653.FR\\', \\'PPC.BING.3.EPR632-DS-PPC-DAL\\', \\'PPC.Google AdWords.3.EPR653.West\\', \\'PPC.Google AdWords.EPR653-GoogleShopping_Product_UK\\', \\'Banner.Banner RTB.3.EPR653.Criteo\\', \\'PPC.BING.3.EPR632.West\\', \\'PPC.BING.3.EPR632.DAL\\'];var generalCookieDomain = \\'.apress.com\\';/*]]>*/</script><script type=\"text/javascript\" src=\"https://www.google.com/recaptcha/api.js\"></script></head><body class=\"cms  cms-client-apress cms-lang-in\"><div><a class=\"link\" id=\"print-header\" href=\"http://www.apress.com\"><img src=\"/spcom/sites/apress/images/logo-print.png\" />www.apress.com</a></div><div class=\"page-wrapper\"><header><script type=\"text/javascript\" src=\"/spcom/min/0.102.0/apress.header.prod.js\" defer=\"defer\"></script><div class=\"branding-container\"><div class=\"row\"><div class=\"columns small-6 large-4 logo-and-changing\"><a class=\"brand\" href=\"/\" title=\"Apress\"><img src=\"/spcom/sites/apress/images/logo.svg\" alt=\"Apress home page\" /></a></div><nav role=\"presentation\" class=\"columns small-6 hide-for-large meta\"><ul><li class=\"meta__search\"><a href=\"https://www.apress.com/searching\" class=\"search-M\">Search</a></li><li class=\"meta__menu\"><a href=\"#menu\" class=\"menu icon\">Menu</a></li></ul></nav><nav role=\"presentation\" class=\"columns large-8 small-12 user\"><ul class=\"menu--primary off-site\"><li class=\"account search-L\"><a href=\"https://www.apress.com/searching\">Search</a></li></ul></nav></div></div><div class=\"navi-container\"><nav class=\"row main\"><ul class=\"menu--primary columns large-8\" id=\"mainNavigation\"><li><a href=\"#\">Categories</a><ul class=\"menu--secondary\"><li class=\"\"><a href=\"/in/apple\">Apple &amp; iOS</a></li><li class=\"\"><a href=\"/in/big-data\">Big Data &amp; Analytics</a></li><li class=\"\"><a href=\"/in/business\">Business</a></li><li class=\"\"><a href=\"/in/databases\">Databases</a></li><li class=\"\"><a href=\"/in/enterprise-software\">Enterprise Software</a></li><li class=\"\"><a href=\"/in/fintech-blockchain-digital-currencies\">Fintech &amp; Blockchain</a></li><li class=\"\"><a href=\"/in/game-development\">Game Development</a></li><li class=\"\"><a href=\"/in/graphics\">Graphics</a></li><li class=\"\"><a href=\"/in/hardware-maker\">Hardware &amp; Maker</a></li><li class=\"\"><a href=\"/in/java\">Java</a></li><li class=\"\"><a href=\"/in/machine-learning\">Machine Learning</a></li><li class=\"\"><a href=\"/in/microsoft\">Microsoft &amp; .NET</a></li><li class=\"\"><a href=\"/in/mobile\">Mobile</a></li><li class=\"\"><a href=\"/in/networking\">Networking &amp; Cloud</a></li><li class=\"\"><a href=\"/in/open-source\">Open Source</a></li><li class=\"\"><a href=\"/in/programming\">Programming</a></li><li class=\"\"><a href=\"/in/python\">Python</a></li><li class=\"\"><a href=\"/in/security\">Security</a></li><li class=\"\"><a href=\"/in/web-development\">Web Development</a></li><li class=\"\"><a href=\"/in/services/publishing-partners\">Publishing Partners</a></li></ul></li><li><a href=\"#\">Services</a><ul class=\"menu--secondary\"><li class=\"\"><a href=\"/in/about\">About Us</a></li><li class=\"\"><a href=\"/in/services/academics\">Academics</a></li><li class=\"\"><a href=\"/in/shop/affiliate/springer-nature\">Affiliate Program</a></li><li class=\"\"><a href=\"/in/services/book-reviewers\">Book Reviewers</a></li><li class=\"\"><a href=\"/in/services/corporate-sales\">Corporate Sales</a></li><li class=\"\"><a href=\"https://www.apress.com/customer-support\">Customer Support</a></li><li class=\"\"><a href=\"/in/services/events\">Events</a></li><li class=\"\"><a href=\"/in/services/rights-permission\">Rights &amp; Permissions</a></li><li class=\"\"><a href=\"/in/services/source-code\">Source Code</a></li><li class=\"\"><a href=\"/in/services/errata\">Submit Errata</a></li><li class=\"\"><a href=\"/in/services/tech-review\">Technical Reviewers</a></li></ul></li><li><a href=\"/in/apress-open\">Apress Open</a></li><li><a href=\"/in/blog\">Blog</a></li><li><a href=\"/in/write-for-us\">Write</a></li><li><a href=\"#\">Shop</a><ul class=\"menu--secondary\"><li class=\"\"><a href=\"/in/shop\">Shop Apress.com</a></li><li class=\"\"><a href=\"/in/services/booksellers\">Booksellers</a></li><li class=\"\"><a href=\"/in/services/bulk-sales\">Bulk Sales ↗</a></li><li class=\"\"><a href=\"/in/services/corporate-sales\">Corporate Sales</a></li><li class=\"\"><a href=\"/in/services/librarians\">Librarians</a></li></ul></li></ul><div class=\"columns small-12 large-3 large-offset-1 search\"><a href=\"https://www.apress.com/searching\">Search</a></div></nav></div><script type=\"text/javascript\" src=\"/spcom/min/apress.components.js\" defer=\"defer\"></script></header><div id=\"content\"><div id=\"id1\" class=\"\"><div class=\"layout-full-grid row\"><div class=\"column small-12 medium-8 large-9 placement-main\"><div id=\"id3\" class=\"cms-container cms-highlight-0\"><div class=\"row\"><div class=\"columns small-12 \"><div class=\"cms-common cms-article default-table\"><p class=\"taxonomy\"><span class=\"publication-date\">9/3/18</span></p><h1 id=\"c15512052\">Gradient Descent Optimization methods from Deep Learning Perspective</h1><div class=\"cms-richtext\"><p class=\"intro--paragraph\">By Santanu Pattanayak<br /></p><p>It is important to understand and appreciate few key points regarding full batch Gradient Descent and Stochastic gradient descent methods along with their shortcomings so that one can appreciate the need of using different variants of gradient based optimizers in Deep Learning.</p><h1>Elliptical Contours</h1><p>The cost function for a linear neuron with least square error is quadratic. When the cost function is quadratic the direction of gradient in full batch gradient descent method gives the best direction for cost reduction in a linear sense but it doesn’t point to the minimum unless the different elliptical contours of the cost function are circles. Incase of long elliptical contours the gradient components might be large in directions where less change is required and less in directions where more change is required to move to the minimum point. As we can see in the Figure 1 below the gradient at &nbsp;doesn’t point to the direction of the minimum i.e. at point . The problem with this condition is if we take small steps by making the learning rate small then the gradient descent would take a while to converge whereas if we take a big learning rate then the gradients would change directions rapidly in directions where the cost function has curvature, leading to oscillations. The cost function for a multi-layer neural network is not quadratic but mostly a smooth function. However locally such non-quadratic cost function can be approximated by quadratic functions and hence the problems of gradient descent inherent to elliptical contours still prevails for non-quadratic cost functions.</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512114/data/v3\" /><br /></p><p><em>Figure 1. Contour plot for a quadratic cost function with elliptical contours.</em><br /></p><p>The best way to get around this problem is to take larger steps in directions in which gradients are small but consistent and take smaller steps in directions with big but inconsistent gradients. This can be achieved if instead of having a fixed learning rate for all the dimensions we have separate learning rate for each dimension.</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512122/data/v1\" /><br /></p><p><em>Figure 2. Gradient descent for a cost function in one variable.</em></p><p>In the Figure 2 above the cost function between <em>A</em> to <em>C</em> is almost linear and hence gradient descent does well. However, from the point <em>C</em> the curvature of the cost function takes over and hence the gradient at <em>C</em> is not able to keep up with the direction of the change in cost function. Based on the gradient if we take a small learning rate at<em> C</em> we will end up at <em>D</em> which is reasonable enough since it doesn’t overshoot the point of minima. However, a larger step size at <em>C</em> will get us to <em>D’</em> which is not desirable since it\\'s on the other side of the minima. Again, a large step size at <em>D’</em> would get us to E and if the learning rate is not reduced the algorithm tends to toggle between points on either side of the minima, leading to oscillations. When this happens one way to stop this oscillation and achieve convergence is to look at the sign of the gradient <img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512130/data/v6\" />&nbsp;or&nbsp;<img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512134/data/v3\" />&nbsp;in successive iterations and if they are having opposite signs reduce the learning so that the oscillations are reduced. Similarly, if the successive gradients are having the same sign then the learning rate can be accordingly increased. When the cost function is a function of multiple weights then the cost function might have curvatures in some dimensions of the weights while along other dimensions the cost function might be linear. Hence for multivariate cost functions the partial derivative of the cost function with respect to each weight <img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512136/data/v3\" />&nbsp;can be similarly analyzed to update the learning rate for each weight or dimension of the cost function.<br /></p><h1>Non-Convexity of Cost functions</h1><p>The other big problem with neural networks is that the cost functions are mostly non-convex and hence the gradient descent method might get stuck at local minimum points leading to sub-optimal solution. The non-convex nature of the neural network is due to the hidden layer units with non-linear activation functions such as sigmoid. Full batch gradient descent uses the full dataset for the gradient computation. While the same is good for convex cost surfaces it has its own problems in case of non-convex cost functions. For non-convex cost surfaces with full batch gradient the model is going to end up in the minima in its basin of attraction.&nbsp; For if the initialized parameters are in the basic of attraction of a local minima that doesn’t provide good generalization full batch gradient would give a sub-optimal solution.</p><p>With stochastic gradient descent, the noisy gradients computed may force the model out of basin of attraction of the bad local minima i.e. one that doesn’t provide good generalization and place it in a more optimal region. Stochastic gradient descent with single data points produces very random and noisy gradients. Gradients with the mini-batches tend to produce much stable estimates of gradients when compared to that of single data points but still noisier than those produced by the full batches. Ideally the mini-batch size should be carefully chosen such that the gradients are noisy enough to avoid or escape bad local minima points but stable enough to converge at global minima or a local minimum that provides good generalization.</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512126/data/v3\" /><br /></p><p><em>Figure 3. Contour plot showing basins of attraction for Global and Local minima and traversal of paths for gradient descent and Stochastic gradient descent.</em></p><p>In the Figure 3 above the dotted arrows correspond to the path taken by Stochastic gradient descent(SGD) while the continuous arrows correspond to the path taken by full batch gradient descent. Full batch gradient descent computes the actual gradient at a point and hence if it is in the basin of attraction of a poor local minimum, gradient descent almost surely ensures that the local minima <em>L</em> is reached. However, in the case of Stochastic gradient because the gradient is based on only portion of the data and not the full batch hence the gradient direction is only a rough estimate. Since the noisy rough estimate doesn’t always point to the actual gradient at the point <em>C </em>hence stochastic gradient descent may escape the basin of attraction of the local minima and fortunately land in the basin of a global minima. Stochastic gradient descent may escape the global minima basin of attraction too but generally if the basin of attraction is large and the mini-batch size are carefully chosen so that the gradients they produce are moderately noisy Stochastic gradient descent is most likely to reach the global minima <em>G</em> (as in this case) or in general some other optimal minima having a large basin of attraction. For non-convex optimization, there are other heuristics as well such as momentum which when adopted along with Stochastic gradient descent increases the chances of the SGD avoiding shallow local minima. Momentum generally keeps track of the previous gradients through the velocity component. So, if the gradients are steadily pointing towards a good local minimum having a large basin of attraction then the velocity component would be high in the direction of the good local minimum. If the new gradient is noisy and points towards a bad local minimum the velocity component will provide momentum to continue in the same direction and not get influenced by the new gradient too much.</p><h1>Saddle points in the High Dimensional Cost functions</h1><p>Another impediment to optimizing non-convex cost functions is the presence of saddle points. The number of saddle points increases exponentially with the dimensionality increase of the parameter space of a cost function. Saddle points are stationary points (i.e. points where the gradient is zero) but are neither a local minimum or a local maximum point. Since the saddle points are associated with long plateau of points with same cost as that of the saddle point the gradient in the plateau region is either zero or very close to zero. Because of this near zero gradient in all directions gradient based optimizers have a hard time coming out of these saddle points. Mathematically to determine whether a point is a saddle point the eigen values of the Hessian matrix of the cost function can be computed at the given point. If there are both positive and negative Eigen values, then it is a saddle point. Just to refresh our memory of local and global minima test, if all the Eigen values of the Hessian matrix are positive at a stationary point then the point is a global minimum whereas if all the Eigen values of the Hessian Matrix are negative at the stationary point then the point is a global maximum. The Eigen vectors of the Hessian matrix for a cost function gives the directions of change in curvature of the cost function whereas the eigen values denotes the magnitude of the curvature changes along those directions. Also for cost functions with continuous second derivatives the Hessian matrix is symmetrical and hence would always produce orthogonal set of Eigen vector giving mutually orthogonal directions for cost curvature changes. If in all such directions given by Eigen vectors the values of the curvature changes (Eigen values) are positive then the point must be a local minimum, whereas if all the values of curvature changes are negative then the point is a local maximum. This generalization works for cost functions with any input dimensionality whereas the determinant rules for determining extremum points varies with the dimensionality of the input to the cost function. Coming back to saddle point, since the Eigen values are positive for some directions while negative for other directions means the curvature of the cost function increases in directions of Eigen values having positive Eigen value while decreases in those directions of Eigen vectors having negative co-efficient. This nature of the cost surface around a saddle point generally leads to a region of long plateau with near to zero gradient and hence makes life tough for gradient descent methods to escape the plateau of this low gradient. The point (0,0)&nbsp;is a saddle point for the function&nbsp;<em>f (x, y) = x<sup>2</sup> - y<sup>2</sup></em>&nbsp;as we can see from the evaluation below:</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512138/data/v3\" /><br /></p><p>So,&nbsp;<em>(x, y) = (0, 0)</em>&nbsp;is a stationary point.&nbsp;The next thing to compute is the Hessian matrix and evaluate its Eigen values at&nbsp;<em>(x, y) = (0, 0)</em>. The Hessian matrix&nbsp;<em>H f(x, y)</em>&nbsp;is as below:</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512140/data/v3\" /><br /></p><p>So, the Hessian <em>H f(x, y)</em>&nbsp;at all points including&nbsp;<em>(x, y) = (0, 0)</em>&nbsp;is&nbsp;<img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512142/data/v4\" /><br /></p><p>The two eigen values of the&nbsp;<em>H f(x, y)</em>&nbsp;are 2 and -2 corresponding to the Eigen vectors&nbsp;<img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512144/data/v3\" />&nbsp;and&nbsp;<img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512146/data/v3\" />&nbsp;which are nothing but the directions along&nbsp;<em>X&nbsp;</em>and&nbsp;<em>Y&nbsp;</em>axis. Since one Eigen value is positive and the other negative hence&nbsp;<em>(x, y) = (0, 0)</em>&nbsp;is a saddle point. Since the Eigen value is positive along X direction hence with respect to X axis the point (0,0)&nbsp;is a local minima whereas with respect to Y direction it is a local maxima.</p><p><img alt=\"\" title=\"\" src=\"//resource-cms.springernature.com/springer-cms/rest/v1/content/15512128/data/v3\" /><br /></p><p><em>Figure 4. Plot of f(x,y) = x<sup>2</sup> - y<sup>2</sup></em></p><p>The non-convex function <em>f(x,y) = x<sup>2</sup> - y<sup>2&nbsp;</sup></em>is plotted in the Figure 4 above where <em>S</em> is the saddle point at <em>x, y&nbsp;= (0, 0)</em><br /></p><p>For more information kindly refer to the author’s book Pro Deep Learning with TensorFlow</p><p><strong>About the Author</strong></p><p><strong>Santanu Pattanayak</strong>&nbsp;currently works at GE, Digital as a Senior Data Scientist. He has 10 years of overall work experience with&nbsp;six of years of experience in the data analytics/data science field and also has a background in development and database technologies. Prior to joining GE, Santanu worked in companies such as RBS, Capgemini, and IBM. He graduated with a degree in electrical engineering from Jadavpur University, Kolkata and is an avid math enthusiast. Santanu is currently pursuing a master\\'s degree in data science from Indian Institute of Technology (IIT), Hyderabad. He also devotes his time to data science hackathons and Kaggle competitions where he ranks within the top 500 across the globe. Santanu was born and brought up in West Bengal, India and currently resides in Bangalore, India with his wife.<br /></p><p class=\"divider--paragraph\">For more, check out Santanu\\'s book <em><strong><em><strong><a href=\"https://www.apress.com/9781484230954\" target=\"_self\" class=\"is-external\">Pro Deep Learning with TensorFlow</a></strong></em></strong></em>.<br /></p></div></div></div></div></div></div><div class=\"column small-12 medium-4 large-3 placement-sidebar\"></div></div></div></div></div><footer><div><div class=\"footer__links\">    <!-- lang = en -->    <div class=\"row row-narrow\">        <div class=\"column small-12 large-6\">            <div class=\"row\">                <div class=\"column small-12 medium-8\" id=\"column0\">                    <a class=\"footer__links__headline flap--mobile\">Apress A-Z</a>                    <div>                        <div class=\"footer__links__column__links row small-up-2\">                            <div class=\"columns\">                                <ul>                                    <li class=\"\"><a href=\"/gp/apple\">Apple &amp; iOS</a></li><li class=\"\"><a href=\"/gp\">ASP.NET</a></li><li class=\"\"><a href=\"/gp/big-data\">Big Data &amp; Analytics</a></li><li class=\"\"><a href=\"/gp/business\">Business</a></li><li class=\"\"><a href=\"/gp/databases\">Databases</a></li><li class=\"\"><a href=\"/gp/enterprise-software\">Enterprise Software</a></li><li class=\"\"><a href=\"/gp/game-development\">Game Development</a></li><li class=\"\"><a href=\"/gp/graphics\">Graphics</a></li><li class=\"\"><a href=\"/gp/hardware-maker\">Hardware &amp; Maker</a></li><li class=\"\"><a href=\"/gp/java\">Java</a></li><li class=\"\"><a href=\"/gp/machine-learning\">Machine Learning</a></li>                                </ul>                            </div>                            <div class=\"columns\">                                <ul>                                    <li class=\"\"><a href=\"/gp/microsoft\">Microsoft &amp; .NET</a></li><li class=\"\"><a href=\"/gp/mobile\">Mobile</a></li><li class=\"\"><a href=\"/gp/networking\">Networking &amp; Cloud</a></li><li class=\"\"><a href=\"/gp/open-source\">Open Source</a></li><li class=\"\"><a href=\"/gp/programming\">Programming</a></li><li class=\"\"><a href=\"/gp/services/publishing-partners\">Publishing Partners</a></li><li class=\"\"><a href=\"/gp/python\">Python</a></li><li class=\"\"><a href=\"/gp/security\">Security</a></li><li class=\"\"><a href=\"/gp/web-development\">Web Development</a></li><li class=\"\"><a href=\"/gp/services/publishing-partners\">Publishing Partners</a></li>                                </ul>                            </div>                        </div>                    </div>                </div>                <div class=\"column small-12 medium-4\" id=\"column1\">                    <a class=\"footer__links__headline flap--mobile\">Services</a>                    <div>                        <ul class=\"footer__links__column__links\">                            <li class=\"\"><a href=\"/gp/about\">About Us</a></li><li class=\"\"><a href=\"/gp/services/academics\">Academics</a></li><li class=\"\"><a href=\"/gp/shop/affiliate/springer-nature\">Affiliate Program</a></li><li class=\"\"><a href=\"/gp/services/book-reviewers\">Book Reviewers</a></li><li class=\"\"><a href=\"/gp/services/corporate-sales\">Corporate Sales</a></li><li class=\"\"><a href=\"/gp/services/customer-support/buying-in-the-apress-shop\">Customer Support</a></li><li class=\"\"><a href=\"/gp/services/events\">Events</a></li><li class=\"\"><a href=\"/gp/services/news\">News &amp; Newsletter</a></li><li class=\"\"><a href=\"/gp/services/rights-permission\">Rights &amp; Permissions</a></li><li class=\"\"><a href=\"/gp/services/source-code\">Source Code</a></li><li class=\"\"><a href=\"/gp/services/errata\">Submit Errata</a></li><li class=\"\"><a href=\"/gp/services/tech-review\">Technical Reviewers</a></li><li class=\"\"><a href=\"/gp/services/user-groups\">User Groups</a></li>                        </ul>                    </div>                </div>            </div>        </div>        <div class=\"column small-12 large-6\">            <div class=\"row\">                <div class=\"column small-12 medium-4\" id=\"column2\">                    <a class=\"footer__links__headline flap--mobile\">Write for Us</a>                    <div>                        <ul class=\"footer__links__column__links\">                            <li class=\"\"><a href=\"/gp/write-for-us\">Write</a></li><li class=\"\"><a href=\"/gp/write-for-us/submit-a-proposal\">Submit a proposal</a></li><li class=\"\"><a href=\"/gp/write-for-us/publishing-process\">Publishing process</a></li><li class=\"\"><a href=\"/gp/write-for-us/author-services\">Documents for Current Authors</a></li>                        </ul>                    </div>                </div>                <div class=\"column small-12 medium-4\" id=\"column3\">                    <a class=\"footer__links__headline flap--mobile\">Company</a>                    <div>                        <ul class=\"footer__links__column__links\">                            <li class=\"\"><a href=\"/gp/about\">About Us</a></li><li class=\"\"><a href=\"/gp/blog\">Blog</a></li><li class=\"\"><a href=\"/gp/services/events\">Events</a></li><li class=\"\"><a href=\"/gp/services/customer-support/buying-in-the-apress-shop/faq\">FAQ</a></li><li class=\"\"><a href=\"/gp/services/news\">News</a></li><li class=\"\"><a href=\"/gp/services/publishing-partners\">Publishing Partners</a></li><li class=\"\"><a href=\"/gp/services/contact-us\">Contact Us</a></li><li class=\"\"><a href=\"/gp/about/team\">Team</a></li>                        </ul>                    </div>                </div>                <div class=\"column small-12 medium-4\">                    <h6 class=\"footer__links__headline\">&nbsp;</h6>                    <div class=\"footer__links__payment\">                        <i>¶</i>                        <i>¢</i>                        <i>“</i>                        <i>¡</i>                    </div>                    <div class=\"footer__links__trustpilot\">                    </div>                </div>            </div>        </div>    </div></div><div class=\"footer__meta\">    <div class=\"row\">        <div class=\"column small-8 medium-9\">            <ul class=\"footer__meta__links\">                <li><a class=\"footer__meta__logo\" href=\"/\"></a></li>                <li class=\"\"><a role=\"button\" data-cc-action=\"preferences\" onclick=\"return false\" href=\"/gp/legal/cookie-policy\">Your privacy choices/Manage cookies</a></li><li class=\"\"><a href=\"/gp/legal/apress-terms-and-conditions/10875332\">Terms &amp; Conditions</a></li><li class=\"\"><a href=\"/gp/legal\">Privacy Policy</a></li><li class=\"\" id=\"footer-copyright\"><a>© 2024 Apress is part of Springer Nature</a></li><li class=\"\"><a href=\"https://www.springernature.com/ccpa\">Your US state privacy rights ↗</a></li>            </ul>        </div>        <div class=\"column small-4 medium-3\" style=\"padding-left: 0;\">            <ul class=\"footer__meta__social\" style=\"margin: 0px -4px;\">                <li class=\"footer-icon\"><a href=\"https://twitter.com/apress\">1</a></li><li class=\"footer-icon\"><a href=\"https://www.facebook.com/ApressMedia/\">2</a></li><li class=\"footer-icon\"><a href=\"https://www.linkedin.com/company/apress/\">3</a></li><li class=\"footer-icon\"><a href=\"https://www.youtube.com/channel/UCToEnUFnPa1GjN_ovvomCEw\">4</a></li><li class=\"footer-icon\"><a href=\"https://www.instagram.com/apresshq/\">5</a></li>            </ul>        </div>        <style>            /*<![CDATA[*/            .footer__meta__social li { padding: 4px; margin: 0.35714rem 0 0 0rem;}            /*]]>*/        </style>        <script>            var prefsButton = document.querySelector(\\'.footer__meta__links [data-cc-action=\"preferences\"]\\');            prefsButton.addEventListener(\\'keydown\\', function (e) {                if (e.code === \\'Space\\') {                    e.preventDefault();                    prefsButton.click();                }            });        </script>    </div></div></div></footer><div></div><noscript><div id=\"jsnotice\" class=\"prompt-bar\"><p>JavaScript is currently disabled, this site works much better if you<a href=\"http://enable-javascript.com/\">enable JavaScript in your browser</a>.</p></div></noscript><!-- Google Tag Manager --><noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-W7J2NZR&gtm_auth=H9WR51Mt3jYIbQSiz_4-IQ&gtm_preview=env-1&gtm_cookies_win=x;\"height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript><!-- End Google Tag Manager --></body><!--[if lt IE 7]> </html> <![endif]--><!--[if IE 7]> </html> <![endif]--><!--[if IE 8]> </html> <![endif]--><!--[if IE 9]> </html> <![endif]--><!--[if gt IE 9]><!--> </html> <!--<![endif]-->'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After creating the proper function we have our result saved in the variable called content, this aproach\n",
    "# gave us a complete html code what is not desired but it's just for this basic example.  \n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf66b4-138b-4d6f-9e9b-210b25fe32f3",
   "metadata": {},
   "source": [
    "### Scrapping using beatiful soup module\n",
    "##### To get a result cleaned up we can use this module that can help us in our task of scrapping\n",
    "\n",
    "##### This script utilizes the requests and BeautifulSoup packages to crawl the blog page of apress.com in order to:\n",
    "##### + Extract a list of recent blog post titles and their URLs.\n",
    "##### + Extract the plain text content related to each blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e033e35-9cb3-45cf-bca0-1825b3cedee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing modules\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3bd84a9-5cb7-4639-b95a-2809c01798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_mapping(content):\n",
    "    \"\"\"\n",
    "    This function extracts blog post titles and URLs from a response object.\n",
    "\n",
    "    Args:\n",
    "        content (str): String content returned from requests.get.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries with keys 'title' and 'url'.\n",
    "        \n",
    "    We are going to use the h3 attributes so when we checked the web page the h3 are the attribute used for titles\n",
    "    \"\"\"\n",
    "    post_detail_list = []\n",
    "    post_soup = BeautifulSoup(content,\"lxml\")\n",
    "    h3_content = post_soup.find_all(\"h3\")\n",
    "    \n",
    "    for h3 in h3_content:\n",
    "        post_detail_list.append(\n",
    "            {'title':h3.a.get_text(),'url':h3.a.attrs.get('href')}\n",
    "            )\n",
    "    \n",
    "    return post_detail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bb8297-984b-4eac-b39f-977b533b46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to iterate through the list of URL and extract eacho blog post's text\n",
    "\n",
    "def get_post_content(content):\n",
    "    \"\"\"\n",
    "    This function extracts blog post content from a response object.\n",
    "    \n",
    "    Args:\n",
    "        content (str): String content returned from requests.get.\n",
    "    \n",
    "    Returns:\n",
    "        str: Blog content in plain text.\n",
    "    \"\"\"\n",
    "\n",
    "    plain_text = \"\"\n",
    "    text_soup = BeautifulSoup(content,\"lxml\")\n",
    "    para_list = text_soup.find_all(\"div\",\n",
    "                                   {'class':'cms-richtext'})\n",
    "    \n",
    "    for p in para_list[0]:\n",
    "        plain_text += p.getText()\n",
    "    \n",
    "    return plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c662ffb8-5995-45de-8525-3257ed16e43a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By Santanu PattanayakIt is important to understand and appreciate few key points regarding full batch Gradient Descent and Stochastic gradient descent methods along with their shortcomings so that one can appreciate the need of using different variants of gradient based optimizers in Deep Learning.Elliptical ContoursThe cost function for a linear neuron with least square error is quadratic. When the cost function is quadratic the direction of gradient in full batch gradient descent method gives the best direction for cost reduction in a linear sense but it doesn’t point to the minimum unless the different elliptical contours of the cost function are circles. Incase of long elliptical contours the gradient components might be large in directions where less change is required and less in directions where more change is required to move to the minimum point. As we can see in the Figure 1 below the gradient at \\xa0doesn’t point to the direction of the minimum i.e. at point . The problem with this condition is if we take small steps by making the learning rate small then the gradient descent would take a while to converge whereas if we take a big learning rate then the gradients would change directions rapidly in directions where the cost function has curvature, leading to oscillations. The cost function for a multi-layer neural network is not quadratic but mostly a smooth function. However locally such non-quadratic cost function can be approximated by quadratic functions and hence the problems of gradient descent inherent to elliptical contours still prevails for non-quadratic cost functions.Figure 1. Contour plot for a quadratic cost function with elliptical contours.The best way to get around this problem is to take larger steps in directions in which gradients are small but consistent and take smaller steps in directions with big but inconsistent gradients. This can be achieved if instead of having a fixed learning rate for all the dimensions we have separate learning rate for each dimension.Figure 2. Gradient descent for a cost function in one variable.In the Figure 2 above the cost function between A to C is almost linear and hence gradient descent does well. However, from the point C the curvature of the cost function takes over and hence the gradient at C is not able to keep up with the direction of the change in cost function. Based on the gradient if we take a small learning rate at C we will end up at D which is reasonable enough since it doesn’t overshoot the point of minima. However, a larger step size at C will get us to D’ which is not desirable since it's on the other side of the minima. Again, a large step size at D’ would get us to E and if the learning rate is not reduced the algorithm tends to toggle between points on either side of the minima, leading to oscillations. When this happens one way to stop this oscillation and achieve convergence is to look at the sign of the gradient \\xa0or\\xa0\\xa0in successive iterations and if they are having opposite signs reduce the learning so that the oscillations are reduced. Similarly, if the successive gradients are having the same sign then the learning rate can be accordingly increased. When the cost function is a function of multiple weights then the cost function might have curvatures in some dimensions of the weights while along other dimensions the cost function might be linear. Hence for multivariate cost functions the partial derivative of the cost function with respect to each weight \\xa0can be similarly analyzed to update the learning rate for each weight or dimension of the cost function.Non-Convexity of Cost functionsThe other big problem with neural networks is that the cost functions are mostly non-convex and hence the gradient descent method might get stuck at local minimum points leading to sub-optimal solution. The non-convex nature of the neural network is due to the hidden layer units with non-linear activation functions such as sigmoid. Full batch gradient descent uses the full dataset for the gradient computation. While the same is good for convex cost surfaces it has its own problems in case of non-convex cost functions. For non-convex cost surfaces with full batch gradient the model is going to end up in the minima in its basin of attraction.\\xa0 For if the initialized parameters are in the basic of attraction of a local minima that doesn’t provide good generalization full batch gradient would give a sub-optimal solution.With stochastic gradient descent, the noisy gradients computed may force the model out of basin of attraction of the bad local minima i.e. one that doesn’t provide good generalization and place it in a more optimal region. Stochastic gradient descent with single data points produces very random and noisy gradients. Gradients with the mini-batches tend to produce much stable estimates of gradients when compared to that of single data points but still noisier than those produced by the full batches. Ideally the mini-batch size should be carefully chosen such that the gradients are noisy enough to avoid or escape bad local minima points but stable enough to converge at global minima or a local minimum that provides good generalization.Figure 3. Contour plot showing basins of attraction for Global and Local minima and traversal of paths for gradient descent and Stochastic gradient descent.In the Figure 3 above the dotted arrows correspond to the path taken by Stochastic gradient descent(SGD) while the continuous arrows correspond to the path taken by full batch gradient descent. Full batch gradient descent computes the actual gradient at a point and hence if it is in the basin of attraction of a poor local minimum, gradient descent almost surely ensures that the local minima L is reached. However, in the case of Stochastic gradient because the gradient is based on only portion of the data and not the full batch hence the gradient direction is only a rough estimate. Since the noisy rough estimate doesn’t always point to the actual gradient at the point C hence stochastic gradient descent may escape the basin of attraction of the local minima and fortunately land in the basin of a global minima. Stochastic gradient descent may escape the global minima basin of attraction too but generally if the basin of attraction is large and the mini-batch size are carefully chosen so that the gradients they produce are moderately noisy Stochastic gradient descent is most likely to reach the global minima G (as in this case) or in general some other optimal minima having a large basin of attraction. For non-convex optimization, there are other heuristics as well such as momentum which when adopted along with Stochastic gradient descent increases the chances of the SGD avoiding shallow local minima. Momentum generally keeps track of the previous gradients through the velocity component. So, if the gradients are steadily pointing towards a good local minimum having a large basin of attraction then the velocity component would be high in the direction of the good local minimum. If the new gradient is noisy and points towards a bad local minimum the velocity component will provide momentum to continue in the same direction and not get influenced by the new gradient too much.Saddle points in the High Dimensional Cost functionsAnother impediment to optimizing non-convex cost functions is the presence of saddle points. The number of saddle points increases exponentially with the dimensionality increase of the parameter space of a cost function. Saddle points are stationary points (i.e. points where the gradient is zero) but are neither a local minimum or a local maximum point. Since the saddle points are associated with long plateau of points with same cost as that of the saddle point the gradient in the plateau region is either zero or very close to zero. Because of this near zero gradient in all directions gradient based optimizers have a hard time coming out of these saddle points. Mathematically to determine whether a point is a saddle point the eigen values of the Hessian matrix of the cost function can be computed at the given point. If there are both positive and negative Eigen values, then it is a saddle point. Just to refresh our memory of local and global minima test, if all the Eigen values of the Hessian matrix are positive at a stationary point then the point is a global minimum whereas if all the Eigen values of the Hessian Matrix are negative at the stationary point then the point is a global maximum. The Eigen vectors of the Hessian matrix for a cost function gives the directions of change in curvature of the cost function whereas the eigen values denotes the magnitude of the curvature changes along those directions. Also for cost functions with continuous second derivatives the Hessian matrix is symmetrical and hence would always produce orthogonal set of Eigen vector giving mutually orthogonal directions for cost curvature changes. If in all such directions given by Eigen vectors the values of the curvature changes (Eigen values) are positive then the point must be a local minimum, whereas if all the values of curvature changes are negative then the point is a local maximum. This generalization works for cost functions with any input dimensionality whereas the determinant rules for determining extremum points varies with the dimensionality of the input to the cost function. Coming back to saddle point, since the Eigen values are positive for some directions while negative for other directions means the curvature of the cost function increases in directions of Eigen values having positive Eigen value while decreases in those directions of Eigen vectors having negative co-efficient. This nature of the cost surface around a saddle point generally leads to a region of long plateau with near to zero gradient and hence makes life tough for gradient descent methods to escape the plateau of this low gradient. The point (0,0)\\xa0is a saddle point for the function\\xa0f (x, y) = x2 - y2\\xa0as we can see from the evaluation below:So,\\xa0(x, y) = (0, 0)\\xa0is a stationary point.\\xa0The next thing to compute is the Hessian matrix and evaluate its Eigen values at\\xa0(x, y) = (0, 0). The Hessian matrix\\xa0H f(x, y)\\xa0is as below:So, the Hessian H f(x, y)\\xa0at all points including\\xa0(x, y) = (0, 0)\\xa0is\\xa0The two eigen values of the\\xa0H f(x, y)\\xa0are 2 and -2 corresponding to the Eigen vectors\\xa0\\xa0and\\xa0\\xa0which are nothing but the directions along\\xa0X\\xa0and\\xa0Y\\xa0axis. Since one Eigen value is positive and the other negative hence\\xa0(x, y) = (0, 0)\\xa0is a saddle point. Since the Eigen value is positive along X direction hence with respect to X axis the point (0,0)\\xa0is a local minima whereas with respect to Y direction it is a local maxima.Figure 4. Plot of f(x,y) = x2 - y2The non-convex function f(x,y) = x2 - y2\\xa0is plotted in the Figure 4 above where S is the saddle point at x, y\\xa0= (0, 0)For more information kindly refer to the author’s book Pro Deep Learning with TensorFlowAbout the AuthorSantanu Pattanayak\\xa0currently works at GE, Digital as a Senior Data Scientist. He has 10 years of overall work experience with\\xa0six of years of experience in the data analytics/data science field and also has a background in development and database technologies. Prior to joining GE, Santanu worked in companies such as RBS, Capgemini, and IBM. He graduated with a degree in electrical engineering from Jadavpur University, Kolkata and is an avid math enthusiast. Santanu is currently pursuing a master's degree in data science from Indian Institute of Technology (IIT), Hyderabad. He also devotes his time to data science hackathons and Kaggle competitions where he ranks within the top 500 across the globe. Santanu was born and brought up in West Bengal, India and currently resides in Bangalore, India with his wife.For more, check out Santanu's book Pro Deep Learning with TensorFlow.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_post_content(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fcbdd-5c4e-4ef5-ae6b-9d3b458b48cb",
   "metadata": {},
   "source": [
    "##### As we can see now the text of one of the URL's in the scrapping looks better than the first options using regular expressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
